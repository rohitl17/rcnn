{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pydicom\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "import glob\n",
    "import ast\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = 815 ## or 169 out of 198 unique scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_1 = 'GRANULOMA_TRAINING_DATA_ONE/'\n",
    "DIR_2 = 'GRANULOMA_TRAINING_DATA_TWO/'\n",
    "\n",
    "DATA_DIR = '../../GRANULOMA_RADIOLOGIST_LABELS/'\n",
    "CSV_PATH_1 = './Granuloma_Data/Granuloma_Annotation_Complete.csv'\n",
    "\n",
    "\n",
    "ORIG_SIZE = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns = pd.read_csv(os.path.join('../Granuloma/', CSV_PATH_1))\n",
    "anns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns['filename'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(anns['filename'][:815]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_test = anns[TRAIN_SPLIT:]\n",
    "anns= anns[:TRAIN_SPLIT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Mask RCNN\n",
    "# sys.path.append(os.path.join('./', 'Mask_RCNN'))  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_WEIGHTS_PATH = \"../Mask_RCNN/mask_rcnn_coco.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['filename','region_shape_attributes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy = np.load('Normal.npy',mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_png_fps():\n",
    "    png_fps = glob.glob(DATA_DIR+'/*/'+'*.png')\n",
    "    return list(set(png_fps))\n",
    "\n",
    "def parse_dataset(anns): \n",
    "#     image_fps = get_png_fps()\n",
    "    image_fps = [os.path.join(DATA_DIR,fp) for fp in anns['filename']]\n",
    "    image_annotations = {fp: [] for fp in image_fps}\n",
    "    for index, row in anns.iterrows():\n",
    "        x = ast.literal_eval(row[cols]['region_shape_attributes']) # Get dict of bbox coordinates\n",
    "        if(len(x)>0):                                              # If they don't exist don;t add to dataset\n",
    "            fp = os.path.join(DATA_DIR, row['filename'])\n",
    "            image_annotations[fp].append(row[cols])\n",
    "    return image_fps, image_annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectorConfig(Config):\n",
    "    \"\"\"Configuration for training pneumonia detection on the RSNA pneumonia dataset.\n",
    "    Overrides values in the base Config class.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Give the configuration a recognizable name  \n",
    "    NAME = 'Granuloma-Healthy-Attached-cv-optimize'\n",
    "    \n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 2\n",
    "    \n",
    "    BACKBONE = 'resnet50'\n",
    "    \n",
    "    NUM_CLASSES = 2  # background + 1 pneumonia classes\n",
    "    \n",
    "    IMAGE_MIN_DIM = 384\n",
    "    IMAGE_MAX_DIM = 768\n",
    "    RPN_ANCHOR_SCALES = ( 4,16,8,32,64)\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "    MAX_GT_INSTANCES = 6\n",
    "    DETECTION_MAX_INSTANCES = 5\n",
    "    DETECTION_MIN_CONFIDENCE = 0.78  ## match target distribution\n",
    "    DETECTION_NMS_THRESHOLD = 0.01\n",
    "\n",
    "    STEPS_PER_EPOCH = 200\n",
    "\n",
    "config = DetectorConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_annotation = [{\"x\": 0,\"y\" : 0,\"width\" : 0,\"height\" : 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_csv = pd.read_csv('All.csv')\n",
    "all_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = all_csv[['filename','region_attributes','region_shape_attributes']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr[2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = []\n",
    "for x in attr:\n",
    "    d = json.loads(x[1])\n",
    "    for k,v in d.items():\n",
    "        if('PRESENT' in v or '[PRESENT]' in v):\n",
    "            print(k)\n",
    "            if(k=='End On Vessel'):\n",
    "                fname.append(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectorDataset(utils.Dataset):\n",
    "    \"\"\"Dataset class for training pneumonia detection on the RSNA pneumonia dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_fps, image_annotations, orig_height, orig_width,img_indxs):\n",
    "        super().__init__(self)\n",
    "        \n",
    "        # Add classes\n",
    "        self.add_class('Granuloma', 1, 'granul')\n",
    "        \n",
    "        # add images \n",
    "        for i, fp in enumerate(image_fps):\n",
    "            if(type(fp)==str):\n",
    "                annotations = image_annotations[fp]\n",
    "                self.add_image('Granuloma', image_id=img_indxs[i], path=fp, \n",
    "                               annotations=annotations, orig_height=orig_height, orig_width=orig_width)\n",
    "            else:\n",
    "                pass\n",
    "#                 annotations = healthy_annotation\n",
    "#                 self.add_image('Granuloma', image_id=i, path=fp, \n",
    "#                                annotations=annotations, orig_height=orig_height, orig_width=orig_width)\n",
    "            \n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        return info['path']\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        fp = info['path']\n",
    "        \n",
    "        \n",
    "        image = cv2.imread(fp)\n",
    "        image  = cv2.resize(image,(ORIG_SIZE,ORIG_SIZE))\n",
    "        # If grayscale. Convert to RGB for consistency.\n",
    "        if len(image.shape) != 3 or image.shape[2] != 3:\n",
    "            image = np.stack((image,) * 3, -1)\n",
    "\n",
    "            \n",
    "        im_healthy = healthy[image_id]\n",
    "        \n",
    "        im_healthy = cv2.resize(im_healthy,(ORIG_SIZE,ORIG_SIZE))\n",
    "        \n",
    "        image = np.hstack((image,im_healthy))\n",
    "        \n",
    "        \n",
    "        return image\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \n",
    "        info = self.image_info[image_id]\n",
    "\n",
    "        fp = info['path']\n",
    "        \n",
    "\n",
    "\n",
    "        image = cv2.imread(fp)\n",
    "        orig_x, orig_y = image.shape[0], image.shape[1]\n",
    "\n",
    "        annotations = info['annotations']\n",
    "        count = len(annotations)\n",
    "\n",
    "        mask = np.zeros((orig_x, orig_y, count), dtype=np.uint8)\n",
    "        res_mask = np.zeros((ORIG_SIZE, ORIG_SIZE, count), dtype=np.uint8)\n",
    "        class_ids = np.zeros((count,), dtype=np.int32)\n",
    "        \n",
    "                           \n",
    "\n",
    "        res_mask_healthy = np.zeros((ORIG_SIZE, ORIG_SIZE, count), dtype=np.uint8)\n",
    "        \n",
    "        \n",
    "        for i, a in enumerate(annotations):\n",
    "            a = ast.literal_eval(a['region_shape_attributes'])\n",
    "            x = int(a['x'])\n",
    "            y = int(a['y'])\n",
    "            w = int(a['width'])\n",
    "            h = int(a['height'])\n",
    "            mask_instance = mask[:, :, i].copy()\n",
    "            cv2.rectangle(mask_instance, (x, y), (x+w, y+h), 255, -1)\n",
    "            mask_instance = cv2.resize(mask_instance,(ORIG_SIZE,ORIG_SIZE))\n",
    "            res_mask[:, :, i] = mask_instance\n",
    "            \n",
    "            class_ids[i] = 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        res_mask = np.hstack((res_mask,res_mask_healthy))\n",
    "\n",
    "        \n",
    "        \n",
    "        return res_mask.astype(np.bool), class_ids.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns unique elements by maintaing order\n",
    "'''\n",
    "def find_unique(x):\n",
    "    res = []\n",
    "    for i in x:\n",
    "        if i not in res:\n",
    "            res.append(i)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Set Creation\n",
    "image_fps_test, image_annotations_test = parse_dataset(anns_test)\n",
    "\n",
    "\n",
    "## REMOVE THOSE KEYS WHICH DONT HAVE ANY BOUNDING BOX\n",
    "\n",
    "keys = image_annotations_test.keys()           \n",
    "\n",
    "for k in list(keys):\n",
    "    v = image_annotations_test[k]\n",
    "    if(len(v)==0):\n",
    "        print(\"hello\")\n",
    "        print(v)\n",
    "        del(image_annotations_test[k])\n",
    "print(len(image_annotations_test)) \n",
    "\n",
    "\n",
    "image_fps_test = find_unique(image_fps_test)\n",
    "# image_fps_test = list(image_annotations_test.keys()) \n",
    "image_fps_list_test = list(image_fps_test)\n",
    "\n",
    "test_index = [i for i in range(len(image_fps_list_test))]\n",
    "\n",
    "# image_fps_val = [image_fps_list[i] for i in test_index]\n",
    "\n",
    "# print(len(image_fps_val))\n",
    "\n",
    "print(\"Test IDX : \", test_index)\n",
    "\n",
    "\n",
    "dataset_test = DetectorDataset(image_fps_list_test, image_annotations_test, ORIG_SIZE, ORIG_SIZE, test_index)\n",
    "dataset_test.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_fps, image_annotations = parse_dataset(anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_annotations[image_fps[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {fp: [] for fp in image_fps}\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## REMOVE THOSE KEYS WHICH DONT HAVE ANY BOUNDING BOX\n",
    "\n",
    "keys = image_annotations.keys()           \n",
    "\n",
    "for k in list(keys):\n",
    "    v = image_annotations[k]\n",
    "    if(len(v)==0):\n",
    "        print(\"hello\")\n",
    "        print(v)\n",
    "        del(image_annotations[k])\n",
    "print(len(image_annotations)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_fps = list(image_annotations.keys()) \n",
    "image_fps = find_unique(image_fps)\n",
    "image_fps_list = list(image_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = [ 68 , 69 , 70 , 71 , 72 , 73  ,74 , 75  ,76 , 77 , 78 , 79 , 80 , 81,  82 , 83,  84 , 85, 86,  87,  88 , 89,  90,  91,  92 , 93 , 94 , 95,  96,  97,  98 , 99, 100 ,101]\n",
    "\n",
    "image_fps_val = [image_fps_list[i] for i in test_index]\n",
    "\n",
    "image_fps_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = iaa.Sequential([\n",
    "    iaa.OneOf([ ## geometric transform\n",
    "        iaa.Affine(\n",
    "            scale={\"x\": (0.98, 1.02), \"y\": (0.98, 1.04)},\n",
    "            translate_percent={\"x\": (-0.02, 0.02), \"y\": (-0.04, 0.04)},\n",
    "            rotate=(-2, 2),\n",
    "            shear=(-1, 1),\n",
    "        ),\n",
    "        iaa.PiecewiseAffine(scale=(0.001, 0.025)),\n",
    "    ]),\n",
    "    iaa.OneOf([ ## brightness or contrast\n",
    "        iaa.Multiply((0.9, 1.1)),\n",
    "        iaa.ContrastNormalization((0.9, 1.1)),\n",
    "    ]),\n",
    "    iaa.OneOf([ ## blur or sharpen\n",
    "        iaa.GaussianBlur(sigma=(0.0, 0.1)),\n",
    "        iaa.Sharpen(alpha=(0.0, 0.1)),\n",
    "    ]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fold 2\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=0)\n",
    "tt = 0\n",
    "\n",
    "for train_index, test_index in kf.split(image_fps_list):\n",
    "    \n",
    "    if(tt==2):\n",
    "        print(\"\\n\\n\")\n",
    "        print(\"############### FOLD BEGINS {0} ##################################\".format(tt))\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        image_fps_train = [image_fps_list[i] for i in train_index]\n",
    "        image_fps_val = [image_fps_list[i] for i in test_index]\n",
    "        \n",
    "        print(image_fps_val)\n",
    "\n",
    "        print(len(image_fps_train), len(image_fps_val))\n",
    "\n",
    "        print(\"VAL IDX : \", test_index)\n",
    "\n",
    "        dataset_train = DetectorDataset(image_fps_train, image_annotations, ORIG_SIZE, ORIG_SIZE,train_index)\n",
    "        dataset_train.prepare()\n",
    "\n",
    "        dataset_val = DetectorDataset(image_fps_val, image_annotations, ORIG_SIZE, ORIG_SIZE, test_index)\n",
    "        dataset_val.prepare()\n",
    "\n",
    "        model = modellib.MaskRCNN(mode='training', config=config, model_dir='./optimize_model_data/')\n",
    "        model.load_weights(COCO_WEIGHTS_PATH, by_name=True, exclude=[\n",
    "            \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n",
    "            \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "\n",
    "        LEARNING_RATE = 0.006\n",
    "\n",
    "        model.train(dataset_train, dataset_val,\n",
    "                learning_rate=LEARNING_RATE*2,\n",
    "                epochs=2,\n",
    "                layers='heads',\n",
    "                augmentation=None)  ## no need to augment yet\n",
    "\n",
    "        model.train(dataset_train, dataset_val,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                epochs=6,\n",
    "                layers='all',\n",
    "                augmentation=augmentation)\n",
    "\n",
    "        model.train(dataset_train, dataset_val,\n",
    "                learning_rate=LEARNING_RATE/5,\n",
    "                epochs=16,\n",
    "                layers='all',\n",
    "                augmentation=augmentation)\n",
    "        \n",
    "        print(\"OVER\")\n",
    "\n",
    "    tt+=1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
